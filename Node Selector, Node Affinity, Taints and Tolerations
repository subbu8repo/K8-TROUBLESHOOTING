These are used when which pod have to deployed on which nodes and similarly which nodes should avoid certain pods to be schedule on them , which k8s nodes should become unschedublable for all the pod during the
backsups or upgrades
Node Selector is a simple way to constrain pods to nodes with specific labels. It allows you to specify a set of key-value pairs that must match the node's labels for a pod to be scheduled on that node.
for example lets say a pod running a application which will only works which will run only on arm processor linux machine or with AMD processor or on only windows machine now you want your pod to schedule only on 
node with arm processor
so i will put node selector label which will tell the kube scheduler only schedule on node which has the label called which ever mention in the node-name. 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-nginx
spec: 
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template: 
    metadata:
      labels:
        app: my-app
    spec: 
      nodeSelector:
        node-name: arm-worker
      containers:
      - name: nginx-image
        image: subbu8docker/recharge:latest
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: "250m"
            memory: "300Mi"

spec:
    containers:
    - name: my-app
    image: my-image
    nodeSelector:
    disktype: ssd

before this i will make sure one or 2 or certain nodes have arm processor and i add label name by 
kubectl edit node nodename
go to the labels section and add node-name with label name 
so, when certain requirements like only windows, linux amd, arm came from developers 1st i add label in node and in pod i will add node selector


Node Affinity it has two options
1. preffered you are telling scheduller to find exact same node if you find label and if it did not find then schedule it anywhere
2. required works same like nodeselector if match finds exact same node 100%

Node Affinity is a more expressive way to specify rules about the placement of pods relative to nodes' labels. It allows you to specify rules that apply only if certain conditions are met. Usage: Define 
nodeAffinity rules in the pod's YAML definition, specifying required and preferred node selectors.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-nginx
spec: 
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template: 
    metadata:
      labels:
        app: my-app
    spec: 
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                matchExpressions:
                - key: node-name
                  operator: In
                  values:
                  - arm-worker
      containers:
      - name: nginx-image
        image: subbu8docker/recharge:latest
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: "250m"
            memory: "300Mi"

spec:
    containers:
    - name: my-app
    image: my-image
    affinity:
    nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
            - key: disktype
            operator: In
            values:
            - ssd

Node affinity offers more grasnulerlevel of configuration 

Taints
Taints are applied to nodes to repel certain pods. They allow nodes to refuse pods unless the pods have a matching toleration
during upgrades you dont want any pods to schedule on certaion nodes
You can make your node unscheduable or to noexecute state or prefernoschedule are 3 types of popular taints
noschedule is used for backuos and upgrades
preferednoschedule is used when performance(cpu issues, memory issues or some issue with the node) that your still identifying) of the node is declining and in the worst case schedule pod on node unless go with 
the other nodes available
command taint
kubectl taint node nodename key1=value1:noSchedule
kubectl get pods -o wide
kubectl taint nodes node1 disktype=ssd:NoSchedule
check
kubectl edit node masternode(controlnode)
you can see particular node is tainted with onschedule
spec:
    containers:
    - name: my-app
    image: my-image
    tolerations:
    - key: disktype
      operator: Equal
      value: ssd
      effect: NoSchedule

Tolerations
Tolerations are basically an exception that is given to certain pods, for example we have 3 nodes in the noschedule status,we have tainted these 3 nodes but for some reason you still want certain
high priority pods like performance criticle, productioncritical they should definitly run what you can do is to these pods you can add a toleration or exception with which they can still run on the noschedule
or on any tainted node
tolerations:
  - key: "key1"
  operator: "Equal"
  value" "value1"
  effect: "NoSchedule"

Tolerations are applied to pods and allow them to schedule onto nodes with matching taints. They override the effect of taints.
spec:
  containers:
  - name: my-app
    image: my-image
  tolerations:
  - key: disktype
    operator: Equal
    value: ssd
    effect: NoSchedule



